threads unlike multiple process in a thread pcb we have a common address space
multiple context for diffrent thread for example diffrent stack frame
benefits -
 parallelization- of tasks helps them to speed up
 specialization   of speed up due to hot cache
if we write a multiprocess implementation there will be more memory
requirement and another issue is that we require ipc mechanism is more costly
but in threads it can be done with the help of share variable which are part of same
address space.
threads on single cpu
   if(tidle) >2*tctswitch then context switch to hide the idling time
   .ct_switch time for thread is less than ct_ switch time for proces memory mapping
    is no need to be done when we are switch from one thread to another

threads also use a mechanism to communicate with each other

 what do we need to implement thread
   thread data structure
     - identify,keep track of their usage
  mechanism to create and manage threads
  mechanism ensure safety b/w threads and running concurrently in same address space

 concurrency-lead to some problem
   both thread can write data on same address
   so we need a mechanism to mutual exclusion
   eg
    using mutex
   -waiting ->specifying condition before starting
   -condition variable
   -waking up from wait state

   thred creation
     -thread type -data structure
       which contain threadid,pc,sp,stack,register etc
    -fork(proc,args) -it is not  the unix fork
       to create a new thread
       proc - procedure that thred will stat to execute
       args is arguements to be passed
    -join terminate a thread
       if thread is b/w then it first complete its execution
       - thread thread1;
          share_list list ;
         thead1 = fork(safe_insert,4);
        safe_insert(6);
         join(thread1);
         result 4->6->null or may be 6->4->null order is not guarantee
     mutex
       both thread can simontneously waant to update the list this could led to data loss
       mutex is a construct by os if  thread lock mutex no other thread can acees the critical section
       so mutex should contain locked status and owner thread
       -birell's lock api
           lock(m)
           {
           }unlock; //implicit unlock
      -common thread api
            lock(m);
            unlock(m);//explicit call
         --mutex request have no order anyone can be granted mutex
         any one get mutex lock which is in queue or just in the queue
         producer consumer problem
       busy waiting approach is waste full cpu cycle is wasted in this case
       another approach is sleep /wake() or wait/signal
        // consumer
         lock(m)
         {
          while(my_list.notFUll())
           wait(m,listfull);
           print and remove();
        }// unlock;
         lock(m)
         {
          insert(thred_id);
          if(listfull)
           signal(listfull);
         }//unlock
    ---  //condition variable api
        wait(mutex,cond)
           -mutex is automatically released and required on wait
      signal(cond)
        - notify only one thing waiting  on condition
       broadcast(cond)
         notify all waiting threads // but mutex will be given to one thread at a  time
         //
         wait(mutex, cond)
         {
         //automatically release mutex
        //aand go on wait
        // wait ...wait ...wait
        // remove from queue
         //require mutex
         exit the wait operation
         }

        //reader writer problem
          reader -0 or more
          writer - o or 1
          naive approach simply protect the resoourse but it will lead to mutual exclusion
          but it is ok for two reader to  read the resource simontaneously
          // rc read counter nd wt write counter
          if both rc and wc = 0
            rok , wok
          if (read_counter> 0)
            rok
          if (wc == 1)
           rno,wno
          so for this we have to use a auxilarry variable resource counter
          example of reader writer problem
            // readers
              lock(counter_mutex)
              {
               while(resource_counter == -1)
               {
                  wait(counter_mutex, readphase);
                  recource_counter++;
               }//unlock
               // read data
               lock(counter_mutex)
               {
                  resource_counter--;
                  if(resource_counter == 0)
                   signal(writephase);
               }// unlock

            //writer
              lock(counter_mutex)
              {
                while(resource_counter!=0)
                {
                 wait(counter_mutex,writephase);
                 resource_counter = -1;
                }
              }// unlock;
              // write data
               lock(counter_mutex)
               {
                 resource_counter = 0 ;
                 broadcast(read_phase);
                 signal(write_phase);
               } //unlock
          // commmon mistake 
              --keep track of mutex 
              --check you are use lock nd unlock always for shared variable  
              --use single mutex for single resource  
              --dont use signal when broadcast is needed// some process 
                 may have to wait indefinetely        
              --signal correct condition 
              --exection of threads is not in order of singal wakeups call 
              spurious wake ups 
                   -uncessary wakeup  program will execute correctly but cpu cycle
                       will be wasted 
                   --suppose mutex is not unlocked by writer and readers are broacast  by  him to wake up int another core this will transfer all the reader in the mutex wait queue.
                   --sol is that we can issue broadcast after unlock but it will not work in readers case suppose we signal writer after unlocking and other reader acquire lock in b/w will cause inconsistency
                   //deadlock 
                     --each competing thread wait on one other 
                     --if there is a cycle in wait graph there is a deadlock 
                     --deadlock prevention is expensive have check at every lock 
                     --deadlock detection and recovery
                         -sometimes we cant do recovery because fo data from external resources 
                     --at last dont do anything just reeboot ostrich  way 
             //kernel level thread and userlevel thread 
                     --one to one model 
                         each kernel level thread is attached to user level thread 
                         os manageut s all thread but this expensive due to context switching and  limits on policies which make the system limiting its protability 
                     --many to one model  
                        all process thread are link to one kernel level thread and each totally portable  managed user level thread libraries and oshas not insight on what is happening and one block thread can block the entire process 
                      -- many to many model  
                         one or more kernel level thread are associated with one or more user level thread and we can have both bounded and unbounded thread as per the priority of the threads but we need a user level thread manager for that purpose 
                        //scope in multithreading  
                          --process level 
                             user level thread library manages thread within a single process  
                          --system level  
                              system thread manager manages thread for entire process  for ex example if some process need more thread then system scope model can assign more thread to that process but in process level scope thread are equally divided in the different process 
                       //multithreaded pattern  
                          boss-worker pattern 
                               -boss :assign work to worker 
                               -worker :perform entire task  
                          efficiency  1/time per boss order 
                           boss signal directly each worker 
                          -- workers dont need to sync b/w them 
                         now a differntly we can do that    
                              -works can placed by bossc in the system  and worker pick their works and sychronisation is needed b/w them  
                        --worker pool can be static or dynamic  
                         --downside is boss do not know capability of worker who can perform task effieciently 
                         --we can create a variant of pattern by using track of workers perfromance for a particular task with this we can do a better quality of service  but we have to load balancing in system and apart that the performance will improve 
                         /pipeline pattern 
                             we create a pipene of specaialized thread and diferent thread work at diffrent level of process weakest thread will be the bottleneck of system more thread can be assign to that part of the process work 
                         // producer consumer buffer can be use to transfer work 
                         //pipeline management is a overhead 
                       layered pattern 
                           -same as pipeline but we can go up and down       
                           --synchronisation is required process is little bit complex than the pipeline pattern 
Boss-Worker Forumla:

time_to_finish_1_order * ceiling (num_orders / num_concurrent_threads)
Pipeline Forumla:

time_to_finish_first_order + (remaining_orders * time_to_finish_last_stage)




 
